"""
Plan Builder Agent
Gera um plano em linguagem natural para responder √† pergunta do usu√°rio
"""

import os
import time
from openai import OpenAI
from typing import Dict, Any

class PlanBuilderAgent:
    """
    Agente respons√°vel por criar um plano de execu√ß√£o em linguagem natural
    que descreve como o sistema ir√° responder √† pergunta do usu√°rio.
    """
    
    def __init__(self):
        print("\n" + "="*80)
        print("üìã PLAN BUILDER AGENT - GERADOR DE PLANOS")
        print("="*80)
        print("‚úÖ Agente inicializado")
        print("="*80 + "\n")
        
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Carregar roles.json ou roles_local.json
        import json
        from pathlib import Path
        from dotenv import load_dotenv
        
        # Carregar .env para verificar BD_REFERENCE
        project_env = Path(__file__).parent.parent.parent / ".env"
        load_dotenv(project_env)
        
        # Verificar qual roles usar baseado em BD_REFERENCE
        bd_reference = os.getenv("BD_REFERENCE", "Athena")
        
        if bd_reference == "Local":
            roles_file = "roles_local.json"
            print(f"   üîß Plan Builder usando roles_local.json (PostgreSQL 15)")
        else:
            roles_file = "roles.json"
            print(f"   üîß Plan Builder usando roles.json (AWS Athena)")
        
        roles_path = Path(__file__).parent / roles_file
        with open(roles_path, 'r', encoding='utf-8') as f:
            self.roles = json.load(f)
        
        self.model = self.roles.get('model_config', {}).get('model', 'gpt-4o')
    
    def build_plan(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Gera um plano em linguagem natural
        
        Args:
            state: Estado contendo:
                - pergunta: str
                - intent_category: str
                - username: str
                - projeto: str
                
        Returns:
            Estado atualizado com:
                - plan: str (plano em linguagem natural)
                - plan_steps: list (passos do plano)
                - estimated_complexity: str
        """
        
        pergunta = state.get("pergunta", "")
        intent_category = state.get("intent_category", "unknown")
        username = state.get("username", "")
        projeto = state.get("projeto", "")
        
        # Header
        print(f"\n{'='*80}")
        print(f"[PLAN_BUILDER] üìã PLAN BUILDER AGENT - N√ì DE PLANEJAMENTO")
        print(f"{'='*80}")
        
        # Inputs
        print(f"[PLAN_BUILDER] üì• INPUTS:")
        print(f"[PLAN_BUILDER]    üìù Pergunta: {pergunta}")
        print(f"[PLAN_BUILDER]    üìÇ Categoria: {intent_category}")
        print(f"[PLAN_BUILDER]    üë§ Username: {username}")
        print(f"[PLAN_BUILDER]    üìÅ Projeto: {projeto}")
        
        print(f"\n[PLAN_BUILDER] ‚öôÔ∏è  PROCESSAMENTO:")
        
        start_time = time.time()
        
        try:
            # Construir prompt para o GPT usando roles.json
            import json
            system_prompt = f"""{self.roles['system_prompt_intro']} {self.roles['description']}

üéØ OBJETIVO:
{self.roles['objective']}

üîí REGRAS DE SEGURAN√áA:
{self.roles['security_rules']['directive']}

üìä CONTEXTO DO BANCO DE DADOS:
{json.dumps(self.roles['database_context'], indent=2, ensure_ascii=False)}

üìã REGRAS DE PLANEJAMENTO:
{json.dumps(self.roles['planning_rules'], indent=2, ensure_ascii=False)}

‚öôÔ∏è DIRETRIZES DE COMPLEXIDADE:
{json.dumps(self.roles['complexity_guidelines'], indent=2, ensure_ascii=False)}

üí° EXEMPLOS:
{json.dumps(self.roles['examples'], indent=2, ensure_ascii=False)}

‚úì CHECKLIST DE VALIDA√á√ÉO:
{json.dumps(self.roles['validation_checklist'], indent=2, ensure_ascii=False)}

RETORNE APENAS JSON v√°lido no formato:
{json.dumps(self.roles['output_structure'], indent=2, ensure_ascii=False)}"""

            # Verificar se h√° contexto de conversa (projeto ativo)
            conversation_context = state.get("conversation_context", "")
            has_history = state.get("has_history", False)
            
            # Verificar se h√° sugest√£o do usu√°rio vinda do user_proposed_plan
            user_proposed_plan = state.get("user_proposed_plan", "")
            
            if user_proposed_plan:
                print(f"[PLAN_BUILDER]    üí° Sugest√£o do usu√°rio detectada: {user_proposed_plan[:100]}...")
                base_prompt = self.roles['user_prompt_with_suggestion'].format(
                    pergunta=pergunta,
                    intent_category=intent_category,
                    projeto=projeto,
                    user_proposed_plan=user_proposed_plan
                )
            else:
                base_prompt = self.roles['user_prompt_normal'].format(
                    pergunta=pergunta,
                    intent_category=intent_category,
                    projeto=projeto
                )
            
            # Injetar contexto ANTES do prompt se houver hist√≥rico
            if has_history and conversation_context:
                user_prompt = f"{conversation_context}\n\n{base_prompt}"
                print(f"[PLAN_BUILDER]    üìö Contexto adicionado: {len(conversation_context)} caracteres")
            else:
                user_prompt = base_prompt
                print(f"[PLAN_BUILDER]    üí¨ Sem contexto (chat geral ou primeira mensagem)")

            print(f"[PLAN_BUILDER]    ü§ñ Chamando {self.model} para gerar plano...")
            
            temperature = self.roles.get('model_config', {}).get('temperature', 0.3)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=800,
                response_format={"type": "json_object"}
            )
            
            print(f"[PLAN_BUILDER]    ‚úÖ Resposta recebida do GPT-4o")
            
            result_text = response.choices[0].message.content.strip()
            
            # Parse JSON
            import json
            try:
                result = json.loads(result_text)
                print(f"[PLAN_BUILDER]    ‚úÖ JSON parseado com sucesso")
            except json.JSONDecodeError as je:
                print(f"[PLAN_BUILDER]    ‚ùå Erro ao fazer parse do JSON: {je}")
                raise je
            
            plan = result.get("plan", "")
            steps = result.get("steps", [])
            complexity = result.get("estimated_complexity", "m√©dia")
            data_sources = result.get("data_sources", [])
            output_format = result.get("output_format", "texto")
            
            # Tokens usados
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else None
            
            # Calcula tempo de execu√ß√£o
            execution_time = time.time() - start_time
            
            # Output
            print(f"\n{'='*80}")
            print(f"[PLAN_BUILDER] üì§ OUTPUT:")
            print(f"[PLAN_BUILDER]    üìã Plano: {plan}")
            print(f"[PLAN_BUILDER]    üìä Passos ({len(steps)}):")
            for i, step in enumerate(steps, 1):
                print(f"[PLAN_BUILDER]       {i}. {step}")
            print(f"[PLAN_BUILDER]    ‚ö° Complexidade: {complexity}")
            print(f"[PLAN_BUILDER]    üíæ Fontes de dados: {', '.join(data_sources)}")
            print(f"[PLAN_BUILDER]    üìà Formato de sa√≠da: {output_format}")
            print(f"[PLAN_BUILDER]    ‚è±Ô∏è  Tempo de execu√ß√£o: {execution_time:.3f}s")
            print(f"{'='*80}\n")
            
            # Retornar campos processados (metadata ser√° criado pelo history_preferences)
            return {
                "plan": plan,
                "plan_steps": steps,
                "estimated_complexity": complexity,
                "data_sources": data_sources,
                "output_format": output_format,
                "execution_time": execution_time,
                "tokens_used": tokens_used,
                "model_used": self.model,
                # Campos extras para metadata (ser√£o usados pelo history_preferences)
                "prompt_length": len(system_prompt) + len(user_prompt),
                "response_length": len(json.dumps(result))
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            print(f"\n{'='*80}")
            print(f"[PLAN_BUILDER] ‚ùå ERRO NO PROCESSAMENTO:")
            print(f"[PLAN_BUILDER]    üí• {str(e)}")
            print(f"{'='*80}\n")
            
            return {
                "plan": f"Erro ao gerar plano: {str(e)}",
                "plan_steps": [],
                "estimated_complexity": "m√©dia",
                "data_sources": [],
                "output_format": "texto",
                "error_message": str(e),
                "execution_time": execution_time,
                "tokens_used": None,
                "model_used": self.model
            }
